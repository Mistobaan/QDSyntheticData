{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 166440.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/mistral-7b-gsm8k-code-rm'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"reciprocate/mistral-7b-gsm8k-code-rm\",\n",
    "    local_dir=\"mistral-7b-gsm8k-code-rm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /opt/NeMo/examples/nlp/language_modeling/conf/megatron_llama_config.yaml /workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 04:10:22 convert_hf_mistral_7b_to_nemo:151] loading checkpoint ./mistral-7b-gsm8k-code-rm/\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [00:02<00:00,  2.35it/s]\n",
      "Some weights of MistralForCausalLM were not initialized from the model checkpoint at ./mistral-7b-gsm8k-code-rm/ and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[NeMo I 2024-05-11 04:10:26 convert_hf_mistral_7b_to_nemo:155] loaded checkpoint ./mistral-7b-gsm8k-code-rm/\n",
      "[NeMo I 2024-05-11 04:10:26 convert_hf_mistral_7b_to_nemo:197] nemo_config: {'mcore_gpt': True, 'micro_batch_size': 4, 'global_batch_size': 8, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'encoder_seq_length': None, 'max_position_embeddings': 32768, 'num_layers': 32, 'hidden_size': 4096, 'ffn_hidden_size': 14336, 'num_attention_heads': 32, 'init_method_std': 0.02, 'use_scaled_init_method': True, 'hidden_dropout': 0.0, 'attention_dropout': 0.0, 'ffn_dropout': 0.0, 'kv_channels': None, 'apply_query_key_layer_scaling': True, 'normalization': 'rmsnorm', 'layernorm_epsilon': 1e-05, 'do_layer_norm_weight_decay': False, 'make_vocab_size_divisible_by': 128, 'pre_process': True, 'post_process': True, 'persist_layer_norm': True, 'bias': False, 'activation': 'fast-swiglu', 'headscale': False, 'transformer_block_type': 'pre_ln', 'openai_gelu': False, 'normalize_attention_scores': True, 'position_embedding_type': 'rope', 'rotary_percentage': 1.0, 'attention_type': 'multihead', 'share_embeddings_and_output_weights': False, 'overlap_p2p_comm': False, 'batch_p2p_comm': True, 'num_query_groups': 8, 'tokenizer': {'library': 'sentencepiece', 'type': None, 'model': './mistral-7b-gsm8k-code-rm/tokenizer.model', 'vocab_file': None, 'merge_file': None, 'delimiter': None, 'sentencepiece_legacy': False}, 'native_amp_init_scale': 4294967296, 'native_amp_growth_interval': 1000, 'hysteresis': 2, 'fp32_residual_connection': False, 'fp16_lm_cross_entropy': False, 'megatron_amp_O2': False, 'grad_allreduce_chunk_size_mb': 125, 'grad_div_ar_fusion': True, 'gradient_accumulation_fusion': False, 'bias_activation_fusion': False, 'bias_dropout_add_fusion': False, 'masked_softmax_fusion': True, 'get_attention_mask_from_fusion': True, 'seed': 1234, 'resume_from_checkpoint': None, 'use_cpu_initialization': True, 'onnx_safe': False, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'sync_batch_comm': False, 'activations_checkpoint_granularity': None, 'activations_checkpoint_method': None, 'activations_checkpoint_num_layers': None, 'num_micro_batches_with_partial_activation_checkpoints': None, 'activations_checkpoint_layers_per_pipeline': None, 'sequence_parallel': False, 'transformer_engine': True, 'fp8': False, 'fp8_e4m3': False, 'fp8_hybrid': True, 'fp8_margin': 0, 'fp8_interval': 1, 'fp8_amax_history_len': 1024, 'fp8_amax_compute_algo': 'max', 'reduce_amax': True, 'use_emha': False, 'data': {'index_mapping_dir': None, 'data_impl': 'mmap', 'splits_string': '900,50,50', 'seq_length': '${model.encoder_seq_length}', 'skip_warmup': True, 'num_workers': 2, 'dataloader_type': 'single', 'reset_position_ids': False, 'reset_attention_mask': False, 'eod_mask_loss': False, 'validation_drop_last': True, 'no_seqlen_plus_one_input_tokens': False, 'pad_samples_to_global_batch_size': False, 'shuffle_documents': True}, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'optim': {'name': 'fused_adam', 'lr': 0.0002, 'weight_decay': 0.01, 'betas': [0.9, 0.98], 'sched': {'name': 'CosineAnnealing', 'warmup_steps': 500, 'constant_steps': 50000, 'min_lr': 2e-05}}, 'window_size': [None, 0], 'rotary_base': 1000000.0, 'precision': 32}\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-05-11 04:10:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "converting layer 0\n",
      "done layer 0\n",
      "converting layer 1\n",
      "done layer 1\n",
      "converting layer 2\n",
      "done layer 2\n",
      "converting layer 3\n",
      "done layer 3\n",
      "converting layer 4\n",
      "done layer 4\n",
      "converting layer 5\n",
      "done layer 5\n",
      "converting layer 6\n",
      "done layer 6\n",
      "converting layer 7\n",
      "done layer 7\n",
      "converting layer 8\n",
      "done layer 8\n",
      "converting layer 9\n",
      "done layer 9\n",
      "converting layer 10\n",
      "done layer 10\n",
      "converting layer 11\n",
      "done layer 11\n",
      "converting layer 12\n",
      "done layer 12\n",
      "converting layer 13\n",
      "done layer 13\n",
      "converting layer 14\n",
      "done layer 14\n",
      "converting layer 15\n",
      "done layer 15\n",
      "converting layer 16\n",
      "done layer 16\n",
      "converting layer 17\n",
      "done layer 17\n",
      "converting layer 18\n",
      "done layer 18\n",
      "converting layer 19\n",
      "done layer 19\n",
      "converting layer 20\n",
      "done layer 20\n",
      "converting layer 21\n",
      "done layer 21\n",
      "converting layer 22\n",
      "done layer 22\n",
      "converting layer 23\n",
      "done layer 23\n",
      "converting layer 24\n",
      "done layer 24\n",
      "converting layer 25\n",
      "done layer 25\n",
      "converting layer 26\n",
      "done layer 26\n",
      "converting layer 27\n",
      "done layer 27\n",
      "converting layer 28\n",
      "done layer 28\n",
      "converting layer 29\n",
      "done layer 29\n",
      "converting layer 30\n",
      "done layer 30\n",
      "converting layer 31\n",
      "done layer 31\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:251] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:257] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:262] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:265] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:282] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:285] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:286] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:297] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:298] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:308] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:312] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:313] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:342] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:354] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:360] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:361] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:362] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_init:363] Rank 0 has embedding rank: 0\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-05-11 04:10:27 tokenizer_utils:191] Getting SentencePiece with model: /home/ubuntu/mistral-7b-gsm8k-code-rm/tokenizer.model\n",
      "[NeMo I 2024-05-11 04:10:27 megatron_base_model:544] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:1109] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:460] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: add_qkv_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: rotary_interleaved in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-11 04:10:27 megatron_base_model:516] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2024-05-11 04:11:51 convert_hf_mistral_7b_to_nemo:336] NeMo model saved to: mistral-7b-ksm8k-code-rm.nemo\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!/usr/bin/python /workspace/convert_hf_mistral_7b_to_nemo.py --in-file ./mistral-7b-gsm8k-code-rm/ --out-file=mistral-7b-ksm8k-code-rm.nemo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /home/ubuntu/mistral-7b-ksm8k-code-rm.nemo /workspace/workspace/reward_model_server/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t\t\t  model-00005-of-00006.safetensors\n",
      "added_tokens.json\t\t  model-00006-of-00006.safetensors\n",
      "config.json\t\t\t  model.safetensors.index.json\n",
      "model-00001-of-00006.safetensors  special_tokens_map.json\n",
      "model-00002-of-00006.safetensors  tokenizer.json\n",
      "model-00003-of-00006.safetensors  tokenizer.model\n",
      "model-00004-of-00006.safetensors  tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls ./mistral-7b-gsm8k-code-rm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/NeMo/scripts/nlp_language_modeling/:\n",
      "augment-text.py\t\t\t  convert_nemo_mistral_7b_to_hf.py\n",
      "build_index_memmap_data.py\t  convert_nemo_mixtral_to_hf.py\n",
      "build_knn_map_index.py\t\t  convert_prompt_learning_ckpt_to_nemo.py\n",
      "build_regex_tokenizer.py\t  convert_starcoder_hf_to_nemo.py\n",
      "build_retrieval_index.py\t  exam_knn_map_quality.py\n",
      "conf\t\t\t\t  extract_inference_only_weights.py\n",
      "convert_hf_falcon_to_nemo.py\t  hf_t5-v1_1_to_nemo.py\n",
      "convert_hf_llama_to_nemo.py\t  hf_t5v1_1_base_config.yaml\n",
      "convert_hf_mistral_7b_to_nemo.py  merge_lora_weights\n",
      "convert_hf_mixtral_to_nemo.py\t  niv2\n",
      "convert_mpt_7b_hf_to_nemo.py\t  preprocess_data_for_megatron.py\n",
      "convert_nemo_falcon_to_hf.py\t  service_launch_scripts\n",
      "convert_nemo_gpt_to_mcore.py\t  sft\n",
      "convert_nemo_llama_to_hf.py\t  t0\n",
      "\n",
      "/workspace/:\n",
      "README.md\t\t\t  install.md\t    papers\t      src\n",
      "categories.json\t\t\t  notebooks\t    requirements.txt  workspace\n",
      "convert_hf_mistral_7b_to_nemo.py  paper_example.md  scripts\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/NeMo/scripts/nlp_language_modeling/ /workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
