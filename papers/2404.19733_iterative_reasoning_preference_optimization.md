# [Iterative Reasoning Preference Optimization](https://arxiv.org/abs/2404.19733)

## Summary

Iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps that lead to the correct answer.

On each iteration we sample multiple chain-of-thought reasoning steps and final answers over training prompts, and then construct preference pairs such that pair winners have correct answers and pair losers have wrong answers.


## Relevance to survey topic (1-5)

Relevance: 4

## Algorithms

Given:
- an initial model $M_0$
- a training set $D = {x_i, y_i}$ containing questions $x_i$ and their correct answers $y_i$

The model is trained for $T$ iterations and updated at each iteration, resulting in models: $M_0, M_1, \dots M_T$.
Given the current model $M_t$, we generate $N$ different responses for every input $x$, where each response consists of CoT reasoning $c$ followed by a final answer $y$

$$(c^n_i , y^n_i ) ∼ M_t(x_i) \text{ for all } x_i ∈ D \text{ and } n ∈ [1, N ]$$

for each answer $y^n_i$ compute a reward value:
$$r^n_i = R(y^n_i , y^i)$$
That can be as simple as $r^n_i = 1$ if $y^n_i = y_i$, and 0 otherwise;

final dataset is: 
$$G_i = \{c^n_i , y^n_i , r^n_i \} \text{ with } n ∈ [1,N]$$

Preference Optimization Dataset:

Split the G_i dataset in G-winners (r=1) and G-losers (r=0), combine all pairs from these dataset to create 

$$ D^{pairs}_t = \{(c_i^{w_k},y_i^{w_k}), (c_i^{l_k},y_i^{l_k})\}$$

Train 

$$
\begin{equation}
\begin{split}
Loss_{NLL + DPO} & = Loss_{NLL}(x_i, c^w_i , y^w_i ) + \alpha Loss_{DPO}(c^w_i, y^w_ i, c^l_i, y^l_i|x_i) \\
                & = − \frac{log M_θ(x_i, c^w_i, y^w_i)}{|x_i| + |c^w_i | + |y^w_i|} + \\
                & − \alpha log \sigma \frac{\beta log M_{\theta}(cw i , yw i |xi)}{
                            log M_t(c^w_i, y^w_i |x_i)} − \beta \frac{log M_{\theta}(c^l_i, y^l_i|x_i)}{log M_t(c^l_i, y^l_i|x_i) } \\
\end{split}
\end{equation}
$$

## Experiments

In total, we performed 4 iterations, producing models M1, M2, M3 and M4. 
For each iteration, we train a maximum of 5000 steps, then select the best checkpoint using a held-out 1k samples from the training set. 
We then retrain including those 1k samples for the selected number of steps. The coefficient α is tuned in {0.5, 1, 2, 4} when training M1, and we end up using 1 for all experiments in the paper. We used a batch size of 16 and a learning rate 7e-7.
In each iteration, we generate N = 30 solutions per problem using sampling with temperature 0.8 for iterations 1-2 and temperature 1.3 for iterations 3-4.

## Benchmarks

- GSM8K

## Metric Results

Iterative RPO outperforms zero-shot CoT, supervised finetuning (SFT) on the gold CoT solutions and variants of DPO by a wide margin.
However, the gain decays across the iterations (17.5%, 4.9%, 3.1%, 0.5%), indicating an upper limit on learning across iterations, especially as we are iterating across a fixed number of prompts, i.e., only from the training samples.

## Other comments

Preference optimization allows the use of negative examples of reasoning chains and answers, which we show improves performance.

## Paper Tags

1. ALGO
